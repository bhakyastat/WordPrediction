{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22aab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get(\"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55280c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text[:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844118e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.text.split('\\n')\n",
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a378ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[253:]\n",
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69369cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" \".join(data)\n",
    "data[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "tokens = clean_text(data)\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    lines.append(line)\n",
    "    if i > 200000:\n",
    "        break\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7652f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]\n",
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dab39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ceee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4039e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b652a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c821929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_text=lines[12343]\n",
    "seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21efb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "    text = []\n",
    "    \n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "    \n",
    "     y_predict = model.predict_classes(encoded)\n",
    "    \n",
    "      predicted_word = ''\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "          if index == y_predict:\n",
    "        predicted_word = word\n",
    "        break\n",
    "    \n",
    "seed_text = seed_text + ' ' + predicted_word\n",
    "text.append(predicted_word)\n",
    "return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cdd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedacaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd771a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d0dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
